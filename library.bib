% Encoding: UTF-8

@InCollection{Meneses2012,
  author    = {Luis Meneses and Richard Furuta and Frank Shipman},
  title     = {{I}dentifying {\textquotedblleft}{S}oft 404{\textquotedblright} {E}rror {P}ages: {A}nalyzing the {L}exical {S}ignatures of {D}ocuments in {D}istributed {C}ollections},
  booktitle = {Theory and Practice of Digital Libraries},
  publisher = {Springer Berlin Heidelberg},
  year      = {2012},
  pages     = {197--208},
  comment   = {The first and the second one of two I could find on soft 404 pages. Whole paper is about soft 404 this time.},
  doi       = {10.1007/978-3-642-33290-6_22},
}

@InProceedings{Bar-Yossef2004,
  author    = {Ziv Bar-Yossef and Andrei Z. Broder and Ravi Kumar and Andrew Tomkins},
  title     = {Sic transit gloria telae},
  booktitle = {Proceedings of the 13th conference on World Wide Web - {WWW} {\textquotesingle}04},
  year      = {2004},
  publisher = {{ACM} Press},
  comment   = {The first paper addressing the soft 404 pages problem and providing the detection algorithm.},
  doi       = {10.1145/988672.988716},
}

@InProceedings{Ho,
  author    = {Tin Kam Ho},
  title     = {Random decision forests},
  booktitle = {Proceedings of 3rd International Conference on Document Analysis and Recognition},
  year      = {1995},
  month     = aug,
  publisher = {{IEEE} Comput. Soc. Press},
  comment   = {The first paper on random forest ever. 10-20-40 fully grown trees in ensemble.},
  doi       = {10.1109/icdar.1995.598994},
}

@Article{Friedman2001,
  author    = {Jerome H. Friedman},
  title     = {{G}reedy function approximation: {A} gradient boosting machine.},
  journal   = {The Annals of Statistics},
  year      = {2001},
  volume    = {29},
  number    = {5},
  pages     = {1189--1232},
  month     = {oct},
  comment   = {44 pages on gradient boosting technique and its applications. Probably the most definite guide to gradient boosting in general.},
  doi       = {10.1214/aos/1013203451},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Breiman1996,
  author    = {Leo Breiman},
  title     = {{B}agging {P}redictors},
  journal   = {Machine Learning},
  year      = {1996},
  volume    = {24},
  number    = {2},
  pages     = {123--140},
  comment   = {One of the first and probably one of the most definite articles about bagging as a technique (and as well may be first to use the term). 16 pages of technique description and theoretical+empirical proof. Also introduces Tree Bagging model.},
  doi       = {10.1023/a:1018054314350},
  publisher = {Springer Nature},
}

@InProceedings{liu2016ssd,
  author       = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C},
  title        = {{SSD}: {S}ingle {S}hot {M}ulti{B}ox {D}etector},
  booktitle    = {European conference on computer vision},
  year         = {2016},
  pages        = {21--37},
  organization = {Springer},
  comment      = {The famous real time object detection net, which is achieving (and sometimes beating) quality of predicitons of Faster R-CNN. 59 FPS with mAP 74.3% on VOC2007 test, vs. Faster R-CNN 7 FPS with mAP 73.2% or YOLO 45 FPS with mAP 63.4%.},
  url          = {https://arxiv.org/pdf/1512.02325.pdf},
}

@InProceedings{krizhevsky2012imagenet,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  title     = {{I}mage{N}et {C}lassification with {D}eep {C}onvolutional {N}eural {N}etworks},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2012},
  editor    = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  pages     = {1097--1105},
  comment   = {Arguably the first great success of deep learning in CV and overall.},
  url       = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
}

@Article{Geurts2006,
  author    = {Pierre Geurts and Damien Ernst and Louis Wehenkel},
  title     = {Extremely randomized trees},
  journal   = {Machine Learning},
  year      = {2006},
  volume    = {63},
  number    = {1},
  pages     = {3--42},
  month     = {mar},
  comment   = {40 pages, introducting Extremely Randomized Trees and studying their properties and dependence on hyperparameters.},
  doi       = {10.1007/s10994-006-6226-1},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{DBLP:journals/corr/Glasmachers17,
  author        = {Tobias Glasmachers},
  title         = {Limits of {E}nd-to-{E}nd {L}earning},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1704.08305},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/Glasmachers17},
  eprint        = {1704.08305},
  timestamp     = {Mon, 13 Aug 2018 16:46:33 +0200},
  url           = {http://arxiv.org/abs/1704.08305},
}

@Article{Erhan2013,
  author      = {Dumitru Erhan and Christian Szegedy and Alexander Toshev and Dragomir Anguelov},
  title       = {Scalable {O}bject {D}etection using {D}eep {N}eural {N}etworks},
  abstract    = {Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.},
  comment     = {The first object detection model to use classifier on top of the class-unaware object detector. Achieved VOC 2017 AP of 0.29 with much higher performance compared to the state-of-the-art.},
  date        = {2013-12-08},
  eprint      = {http://arxiv.org/abs/1312.2249v1},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1312.2249v1:PDF},
  keywords    = {cs.CV, stat.ML},
}

@Article{Simonyan2014,
  author      = {Karen Simonyan and Andrew Zisserman},
  title       = {Very {D}eep {C}onvolutional {N}etworks for {L}arge-{S}cale {I}mage {R}ecognition},
  abstract    = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  comment     = {The famous VGG nets, which are used widely as a backbone for modern NNs.},
  date        = {2014-09-04},
  eprint      = {http://arxiv.org/abs/1409.1556v6},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1409.1556v6:PDF},
  keywords    = {cs.CV},
}

@Comment{jabref-meta: databaseType:bibtex;}
