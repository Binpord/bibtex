% Encoding: UTF-8

@InProceedings{10.1007/978-3-642-33290-6_22,
  author    = {Meneses, Luis and Furuta, Richard and Shipman, Frank},
  title     = {Identifying ``Soft 404'' Error Pages: Analyzing the Lexical Signatures of Documents in Distributed Collections},
  booktitle = {Theory and Practice of Digital Libraries},
  year      = {2012},
  editor    = {Zaphiris, Panayiotis and Buchanan, George and Rasmussen, Edie and Loizides, Fernando},
  pages     = {197--208},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Collections of Web-based resources are often decentralized; leaving the task of identifying and locating removed resources to collection managers who must rely on http response codes. When a resource is no longer available, the server is supposed to return a 404 error code. In practice and to be friendlier to human readers, many servers respond with a 200 OK code and indicate in the text of the response that the document is no longer available. In the reported study, 3.41{\%} of servers respond in this manner. To help collection managers identify these ``friendly'' or ``soft'' 404s, we developed two methods that use a Na{\"i}ve Bayes classifier based on known valid responses and known 404 responses. The classifier was able to predict soft 404 pages with a precision of 99{\%} and a recall of 92{\%}. We will also elaborate on the results obtained from our study and will detail the lessons learned.},
  isbn      = {978-3-642-33290-6},
}

@Article{Goodfellow2014,
  author      = {Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
  title       = {Explaining and Harnessing Adversarial Examples},
  abstract    = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  date        = {2014-12-20},
  eprint      = {http://arxiv.org/abs/1412.6572v3},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1412.6572v3:PDF},
  keywords    = {stat.ML, cs.LG},
}

@Comment{jabref-meta: databaseType:bibtex;}
