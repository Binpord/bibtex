% Encoding: UTF-8

@Article{Girshick2013,
  author      = {Ross Girshick and Jeff Donahue and Trevor Darrell and Jitendra Malik},
  title       = {Rich feature hierarchies for accurate object detection and semantic segmentation},
  abstract    = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012---achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.},
  date        = {2013-11-11},
  eprint      = {http://arxiv.org/abs/1311.2524v5},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1311.2524v5:PDF},
  keywords    = {cs.CV},
}

@Article{Sermanet2013,
  author      = {Pierre Sermanet and David Eigen and Xiang Zhang and Michael Mathieu and Rob Fergus and Yann LeCun},
  title       = {Over{F}eat: {I}ntegrated {R}ecognition, {L}ocalization and {D}etection using {C}onvolutional {N}etworks},
  abstract    = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
  date        = {2013-12-21},
  eprint      = {http://arxiv.org/abs/1312.6229v4},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1312.6229v4:PDF},
  keywords    = {cs.CV},
}

@Article{He2014,
  author      = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  title       = {Spatial {P}yramid {P}ooling in {D}eep {C}onvolutional {N}etworks for {V}isual {R}ecognition},
  abstract    = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224x224) input image. This requirement is "artificial" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102x faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.},
  date        = {2014-06-18},
  doi         = {10.1007/978-3-319-10578-9_23},
  eprint      = {http://arxiv.org/abs/1406.4729v4},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1406.4729v4:PDF},
  keywords    = {cs.CV},
}

@Article{Girshick2015,
  author      = {Ross Girshick},
  title       = {Fast {R-CNN}},
  abstract    = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  date        = {2015-04-30},
  eprint      = {http://arxiv.org/abs/1504.08083v2},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1504.08083v2:PDF},
  keywords    = {cs.CV},
}

@Article{Ren2015,
  author      = {Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
  title       = {Faster {R-CNN}: {T}owards {R}eal-{T}ime {O}bject {D}etection with {R}egion {P}roposal {N}etworks},
  abstract    = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  date        = {2015-06-04},
  eprint      = {http://arxiv.org/abs/1506.01497v3},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1506.01497v3:PDF},
  keywords    = {cs.CV},
}

@Article{Redmon2015,
  author      = {Joseph Redmon and Santosh Divvala and Ross Girshick and Ali Farhadi},
  title       = {You {O}nly {L}ook {O}nce: {U}nified, {R}eal-{T}ime {O}bject {D}etection},
  abstract    = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  date        = {2015-06-08},
  eprint      = {http://arxiv.org/abs/1506.02640v5},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1506.02640v5:PDF},
  keywords    = {cs.CV},
}

@Article{Yu2015,
  author      = {Fisher Yu and Vladlen Koltun},
  title       = {Multi-{S}cale {C}ontext {A}ggregation by {D}ilated {C}onvolutions},
  abstract    = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
  date        = {2015-11-23},
  eprint      = {http://arxiv.org/abs/1511.07122v3},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1511.07122v3:PDF},
  keywords    = {cs.CV},
}

@Article{Yang2018,
  author      = {Lin Yang and Yizhe Zhang and Zhuo Zhao and Hao Zheng and Peixian Liang and Michael T. C. Ying and Anil T. Ahuja and Danny Z. Chen},
  title       = {Box{N}et: {D}eep {L}earning {B}ased {B}iomedical {I}mage {S}egmentation {U}sing {B}oxes {O}nly {A}nnotation},
  abstract    = {In recent years, deep learning (DL) methods have become powerful tools for biomedical image segmentation. However, high annotation efforts and costs are commonly needed to acquire sufficient biomedical training data for DL models. To alleviate the burden of manual annotation, in this paper, we propose a new weakly supervised DL approach for biomedical image segmentation using boxes only annotation. First, we develop a method to combine graph search (GS) and DL to generate fine object masks from box annotation, in which DL uses box annotation to compute a rough segmentation for GS and then GS is applied to locate the optimal object boundaries. During the mask generation process, we carefully utilize information from box annotation to filter out potential errors, and then use the generated masks to train an accurate DL segmentation network. Extensive experiments on gland segmentation in histology images, lymph node segmentation in ultrasound images, and fungus segmentation in electron microscopy images show that our approach attains superior performance over the best known state-of-the-art weakly supervised DL method and is able to achieve (1) nearly the same accuracy compared to fully supervised DL methods with far less annotation effort, (2) significantly better results with similar annotation time, and (3) robust performance in various applications.},
  date        = {2018-06-02},
  eprint      = {http://arxiv.org/abs/1806.00593v1},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1806.00593v1:PDF},
  keywords    = {cs.CV},
}

@Article{Wu2018,
  author      = {Bichen Wu and Xiaoliang Dai and Peizhao Zhang and Yanghan Wang and Fei Sun and Yiming Wu and Yuandong Tian and Peter Vajda and Yangqing Jia and Kurt Keutzer},
  title       = {{FBN}et: {H}ardware-{A}ware {E}fficient {C}onv{N}et {D}esign via {D}ifferentiable {N}eural {A}rchitecture {S}earch},
  abstract    = {Designing accurate and efficient ConvNets for mobile devices is challenging because the design space is combinatorially large. Due to this, previous neural architecture search (NAS) methods are computationally expensive. ConvNet architecture optimality depends on factors such as input resolution and target devices. However, existing approaches are too expensive for case-by-case redesigns. Also, previous work focuses primarily on reducing FLOPs, but FLOP count does not always reflect actual latency. To address these, we propose a differentiable neural architecture search (DNAS) framework that uses gradient-based methods to optimize ConvNet architectures, avoiding enumerating and training individual architectures separately as in previous methods. FBNets, a family of models discovered by DNAS surpass state-of-the-art models both designed manually and generated automatically. FBNet-B achieves 74.1% top-1 accuracy on ImageNet with 295M FLOPs and 23.1 ms latency on a Samsung S8 phone, 2.4x smaller and 1.5x faster than MobileNetV2-1.3 with similar accuracy. Despite higher accuracy and lower latency than MnasNet, we estimate FBNet-B's search cost is 420x smaller than MnasNet's, at only 216 GPU-hours. Searched for different resolutions and channel sizes, FBNets achieve 1.5% to 6.4% higher accuracy than MobileNetV2. The smallest FBNet achieves 50.2% accuracy and 2.9 ms latency (345 frames per second) on a Samsung S8. Over a Samsung-optimized FBNet, the iPhone-X-optimized model achieves a 1.4x speedup on an iPhone X.},
  date        = {2018-12-09},
  eprint      = {http://arxiv.org/abs/1812.03443v3},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1812.03443v3:PDF},
  keywords    = {cs.CV},
}

@InCollection{Rothe2015,
  author    = {Rasmus Rothe and Matthieu Guillaumin and Luc Van Gool},
  title     = {Non-maximum {S}uppression for {O}bject {D}etection by {P}assing {M}essages {B}etween {W}indows},
  booktitle = {Computer Vision -- {ACCV} 2014},
  publisher = {Springer International Publishing},
  year      = {2015},
  pages     = {290--306},
  doi       = {10.1007/978-3-319-16865-4_19},
}

@InProceedings{glorot2010understanding,
  author    = {Glorot, Xavier and Bengio, Yoshua},
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  year      = {2010},
  pages     = {249--256},
}

@Comment{jabref-meta: databaseType:bibtex;}
